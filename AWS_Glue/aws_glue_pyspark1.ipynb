{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a24128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Deptno: long (nullable = true)\n",
      " |-- Dname: string (nullable = true)\n",
      " |-- Loc: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Deptno=10, Dname='ACCOUNTING', Loc='NEW YORK'),\n",
       " Row(Deptno=20, Dname='RESEARCH', Loc='DALLAS')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate()\n",
    "\n",
    "data=[\n",
    "      (10,'ACCOUNTING','NEW YORK'),\n",
    "(20,'RESEARCH','DALLAS'),\n",
    "(30,'SALES','CHICAGO'),\n",
    "(40,'OPERATIONS','BOSTON')\n",
    "      ]\n",
    "columns=[\"Deptno\",\"Dname\",\"Loc\"]\n",
    "df=spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "print(df.printSchema())\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d1971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|update_date|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "| 7369| SMITH|    CLERK|7902|17-12-1980| 800|null|    20| 2022-01-01|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600| 300|    30| 2022-01-01|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250| 500|    30| 2022-01-01|\n",
      "| 7566| JONES|  MANAGER|7839|04-02-1981|2975|null|    20| 2022-01-05|\n",
      "| 7654|MARTIN| SALESMAN|7698|21-09-1981|1250|1400|    30| 2022-01-03|\n",
      "| 7698|   SGR|  MANAGER|7839|05-01-1981|2850|null|    30| 2022-01-04|\n",
      "| 7782|  RAVI|  MANAGER|7839|06-09-1981|2450|null|    10| 2022-01-02|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-04-1987|3000|null|    20| 2022-01-02|\n",
      "| 7839|  KING|PRESIDENT|null|01-11-1981|5000|null|    10| 2022-01-02|\n",
      "| 7844|TURNER| SALESMAN|7698|09-08-1981|1500|   0|    30| 2022-01-02|\n",
      "| 7876| ADAMS|    CLERK|7788|23-05-1987|1100|null|    20| 2022-01-03|\n",
      "| 7900| JAMES|    CLERK|7698|12-03-1981| 950|null|    30| 2022-01-03|\n",
      "| 7902|  FORD|  ANALYST|7566|12-03-1981|3000|null|    20| 2022-01-03|\n",
      "| 7934|MILLER|    CLERK|7782|01-03-1982|1300|null|    10| 2022-01-03|\n",
      "| 1234|SEKHAR|   doctor|7777|      null| 667|  78|    80| 2022-01-03|\n",
      "| 7369| SMITH|    CLERK|7902|17-12-1980| 800|null|    20| 2022-01-04|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600| 300|    30| 2022-01-04|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250| 500|    30| 2022-01-04|\n",
      "| 7566| JONES|  MANAGER|7839|04-02-1981|2975|null|    20| 2022-01-04|\n",
      "| 7654|MARTIN| SALESMAN|7698|21-09-1981|1250|1400|    30| 2022-01-05|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField, DateType, IntegerType\n",
    "\n",
    "header=[\"EMPNO\",\"ENAME\",\"JOB\",\"MGR\",\"HIREDATE\",\"SAL\",\"COMM\",\"DEPTNO\",\"UPDATED_DATE\"]\n",
    "data_emp = map(lambda r:(r[0],r[1],r[2],r[3],r[4],r[5],r[6],r[7],r[8]),\n",
    "           map(lambda x:x.split(\",\"),\n",
    "[\"7369,SMITH,CLERK,7902,17-12-1980,800,null,20,2022-01-01\",\n",
    "\"7499,ALLEN,SALESMAN,7698,20-02-1981,1600,300,30,2022-01-01\",\n",
    "\"7521,WARD,SALESMAN,7698,22-02-1981,1250,500,30,2022-01-01\",\n",
    "\"7566,JONES,MANAGER,7839,04-02-1981,2975,null,20,2022-01-05\",\n",
    "\"7654,MARTIN,SALESMAN,7698,21-09-1981,1250,1400,30,2022-01-03\",\n",
    "\"7698,SGR,MANAGER,7839,05-01-1981,2850,null,30,2022-01-04\",\n",
    "\"7782,RAVI,MANAGER,7839,06-09-1981,2450,null,10,2022-01-02\",\n",
    "\"7788,SCOTT,ANALYST,7566,19-04-1987,3000,null,20,2022-01-02\",\n",
    "\"7839,KING,PRESIDENT,null,01-11-1981,5000,null,10,2022-01-02\",\n",
    "\"7844,TURNER,SALESMAN,7698,09-08-1981,1500,0,30,2022-01-02\",\n",
    "\"7876,ADAMS,CLERK,7788,23-05-1987,1100,null,20,2022-01-03\",\n",
    "\"7900,JAMES,CLERK,7698,12-03-1981,950,null,30,2022-01-03\",\n",
    "\"7902,FORD,ANALYST,7566,12-03-1981,3000,null,20,2022-01-03\",\n",
    "\"7934,MILLER,CLERK,7782,01-03-1982,1300,null,10,2022-01-03\",\n",
    "\"1234,SEKHAR,doctor,7777,null,667,78,80,2022-01-03\",\n",
    "\"7369,SMITH,CLERK,7902,17-12-1980,800,null,20,2022-01-04\",\n",
    "\"7499,ALLEN,SALESMAN,7698,20-02-1981,1600,300,30,2022-01-04\",\n",
    "\"7521,WARD,SALESMAN,7698,22-02-1981,1250,500,30,2022-01-04\",\n",
    "\"7566,JONES,MANAGER,7839,04-02-1981,2975,null,20,2022-01-04\",\n",
    "\"7654,MARTIN,SALESMAN,7698,21-09-1981,1250,1400,30,2022-01-05\",\n",
    "\"7698,SGR,MANAGER,7839,05-01-1981,2850,null,30,2022-01-05\",\n",
    "\"7782,RAVI,MANAGER,7839,06-09-1981,2450,null,10,2022-01-05\",\n",
    "\"7788,SCOTT,ANALYST,7566,19-04-1987,3000,null,20,2022-01-06\",\n",
    "\"7839,KING,PRESIDENT,null,01-11-1981,5000,null,10,2022-01-06\",\n",
    "\"7844,TURNER,SALESMAN,7698,09-08-1981,1500,0,30,2022-01-06\",\n",
    "\"7876,ADAMS,CLERK,7788,23-05-1987,1100,null,20,2022-01-06\",\n",
    "\"7900,JAMES,CLERK,7698,12-03-1981,950,null,30,2022-01-07\",\n",
    "\"7902,FORD,ANALYST,7566,12-03-1981,3000,null,20,2022-01-07\",\n",
    "\"7934,MILLER,CLERK,7782,01-03-1982,1300,null,10,2022-01-07\",\n",
    "\"1234,RAM,CLERK,7457,null,494,588,80,2022-01-07\"\n",
    "]))\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"empno\",StringType(),nullable=True),\n",
    "    StructField(\"ename\",StringType(),nullable=True),\n",
    "    StructField(\"job\",StringType(),nullable=False),\n",
    "    StructField(\"mgr\",StringType(),nullable=False),\n",
    "    StructField(\"hiredate\",StringType(),nullable=False),\n",
    "    StructField(\"sal\",StringType(),nullable=True),\n",
    "    StructField(\"comm\",StringType(),nullable=False),\n",
    "    StructField(\"deptno\",StringType(),nullable=True),\n",
    "    StructField(\"update_date\",StringType(),nullable=False)\n",
    "])\n",
    "\n",
    "df_emp=spark.createDataFrame(data_emp, schema=schema)\n",
    "\n",
    "df_emp.show()\n",
    "#df_emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999ec9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0),\n",
       " Row(city='Paris', type='Food', price=8.0),\n",
       " Row(city='Paris', type='Cloting', price=15.0),\n",
       " Row(city='Marseille', type='Food', price=20.0),\n",
       " Row(city='Lyon', type='Book', price=10.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header=[\"city\",\"type\",\"price\"]\n",
    "data=map(lambda r:(r[0], r[1], float(r[2])),\n",
    "         map(lambda x:x.split(\",\"),\n",
    "            [\"Paris,Food,19.0\",\"Marseille,Clothing,12.00\",\n",
    "             \"Paris,Food,8.00\",\"Paris,Cloting,15.00\",\n",
    "             \"Marseille,Food,20.00\",\"Lyon,Book,10.00\"\n",
    "            ]))\n",
    "df=spark.createDataFrame(data, header)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5af9bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5583c8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6b06410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 'string'), ('type', 'string'), ('price', 'double')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef627b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris| Cloting| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "\n",
    "data=map(lambda r:(r[0], r[1], float(r[2])),\n",
    "         map(lambda x:x.split(\",\"),\n",
    "            [\"Paris,Food,19.0\",\"Marseille,Clothing,12.00\",\n",
    "             \"Paris,Food,8.00\",\"Paris,Cloting,15.00\",\n",
    "             \"Marseille,Food,20.00\",\"Lyon,Book,10.00\"\n",
    "            ]))\n",
    "schema=StructType(\n",
    "        [ StructField(\"city\",StringType(),nullable=True),\n",
    "        StructField(\"type\",StringType(),nullable=True),\n",
    "        StructField(\"price\",FloatType(),nullable=False),\n",
    "            \n",
    "        ])\n",
    "df=spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71d985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     city|\n",
      "+---------+\n",
      "|    Paris|\n",
      "|Marseille|\n",
      "|    Paris|\n",
      "|    Paris|\n",
      "|Marseille|\n",
      "|     Lyon|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa02ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     city|    type|\n",
      "+---------+--------+\n",
      "|    Paris|    Food|\n",
      "|Marseille|Clothing|\n",
      "|    Paris|    Food|\n",
      "|    Paris| Cloting|\n",
      "|Marseille|    Food|\n",
      "|     Lyon|    Book|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"city\",\"type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e79d9",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61be0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "| city|   type|price|\n",
      "+-----+-------+-----+\n",
      "|Paris|   Food| 19.0|\n",
      "|Paris|   Food|  8.0|\n",
      "|Paris|Cloting| 15.0|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.city=='Paris').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e269b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| city|type|price|\n",
      "+-----+----+-----+\n",
      "|Paris|Food| 19.0|\n",
      "|Paris|Food|  8.0|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.city=='Paris').filter(df.type=='Food').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9da5bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| city|type|price|\n",
      "+-----+----+-----+\n",
      "|Paris|Food| 19.0|\n",
      "|Paris|Food|  8.0|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.city=='Paris') & (df.type=='Food')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4a3cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| city|\n",
      "+-----+\n",
      "|Paris|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.city=='Paris') & (df.price > 18.0)).select('city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b25ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     city|price|\n",
      "+---------+-----+\n",
      "|    Paris|  8.0|\n",
      "|     Lyon| 10.0|\n",
      "|Marseille| 12.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.price < 15).orderBy(df.price.asc()).select(\"city\",\"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f935818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|update_date|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600| 300|    30| 2022-01-01|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250| 500|    30| 2022-01-01|\n",
      "| 7566| JONES|  MANAGER|7839|04-02-1981|2975|null|    20| 2022-01-05|\n",
      "| 7654|MARTIN| SALESMAN|7698|21-09-1981|1250|1400|    30| 2022-01-03|\n",
      "| 7698|   SGR|  MANAGER|7839|05-01-1981|2850|null|    30| 2022-01-04|\n",
      "| 7782|  RAVI|  MANAGER|7839|06-09-1981|2450|null|    10| 2022-01-02|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-04-1987|3000|null|    20| 2022-01-02|\n",
      "| 7839|  KING|PRESIDENT|null|01-11-1981|5000|null|    10| 2022-01-02|\n",
      "| 7844|TURNER| SALESMAN|7698|09-08-1981|1500|   0|    30| 2022-01-02|\n",
      "| 7902|  FORD|  ANALYST|7566|12-03-1981|3000|null|    20| 2022-01-03|\n",
      "| 1234|SEKHAR|   doctor|7777|      null| 667|  78|    80| 2022-01-03|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600| 300|    30| 2022-01-04|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250| 500|    30| 2022-01-04|\n",
      "| 7566| JONES|  MANAGER|7839|04-02-1981|2975|null|    20| 2022-01-04|\n",
      "| 7654|MARTIN| SALESMAN|7698|21-09-1981|1250|1400|    30| 2022-01-05|\n",
      "| 7698|   SGR|  MANAGER|7839|05-01-1981|2850|null|    30| 2022-01-05|\n",
      "| 7782|  RAVI|  MANAGER|7839|06-09-1981|2450|null|    10| 2022-01-05|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-04-1987|3000|null|    20| 2022-01-06|\n",
      "| 7839|  KING|PRESIDENT|null|01-11-1981|5000|null|    10| 2022-01-06|\n",
      "| 7844|TURNER| SALESMAN|7698|09-08-1981|1500|   0|    30| 2022-01-06|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.filter(df_emp.job!='CLERK').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "952672cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     city|price|\n",
      "+---------+-----+\n",
      "|    Paris|  8.0|\n",
      "|     Lyon| 10.0|\n",
      "|Marseille| 12.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.price < 15).sort(df.price.asc()).select(\"city\",\"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c1953",
   "metadata": {},
   "source": [
    "Manipulating Columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eab9632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---+\n",
      "|     city|    type|price|Six|\n",
      "+---------+--------+-----+---+\n",
      "|    Paris|    Food| 19.0|  6|\n",
      "|Marseille|Clothing| 12.0|  6|\n",
      "|    Paris|    Food|  8.0|  6|\n",
      "|    Paris| Cloting| 15.0|  6|\n",
      "|Marseille|    Food| 20.0|  6|\n",
      "|     Lyon|    Book| 10.0|  6|\n",
      "+---------+--------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "\n",
    "df=df.withColumn(\"Six\",lit(6)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80237b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---------+\n",
      "|     city|    type|price|50percent|\n",
      "+---------+--------+-----+---------+\n",
      "|    Paris|    Food| 19.0|      9.5|\n",
      "|Marseille|Clothing| 12.0|      6.0|\n",
      "|    Paris|    Food|  8.0|      4.0|\n",
      "|    Paris| Cloting| 15.0|      7.5|\n",
      "|Marseille|    Food| 20.0|     10.0|\n",
      "|     Lyon|    Book| 10.0|      5.0|\n",
      "+---------+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,FloatType\n",
    "\n",
    "data=map(lambda r:(r[0], r[1], float(r[2])),\n",
    "         map(lambda x:x.split(\",\"),\n",
    "            [\"Paris,Food,19.0\",\"Marseille,Clothing,12.00\",\n",
    "             \"Paris,Food,8.00\",\"Paris,Cloting,15.00\",\n",
    "             \"Marseille,Food,20.00\",\"Lyon,Book,10.00\"\n",
    "            ]))\n",
    "\n",
    "schema=StructType(\n",
    "    [StructField('city', StringType(),nullable=True),\n",
    "    StructField('type',StringType(),nullable=True),\n",
    "    StructField('price',FloatType(),nullable=True),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "df=spark.createDataFrame(data=data, schema=schema)\n",
    "df=df.withColumn(\"50percent\",df.price / 2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186fe6f",
   "metadata": {},
   "source": [
    "# DROPPING COLUMNS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be25220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+------+-----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|deptno|update_date|\n",
      "+-----+------+---------+----+----------+----+------+-----------+\n",
      "| 7369| SMITH|    CLERK|7902|17-12-1980| 800|    20| 2022-01-01|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600|    30| 2022-01-01|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250|    30| 2022-01-01|\n",
      "| 7566| JONES|  MANAGER|7839|04-02-1981|2975|    20| 2022-01-05|\n",
      "| 7654|MARTIN| SALESMAN|7698|21-09-1981|1250|    30| 2022-01-03|\n",
      "| 7698|   SGR|  MANAGER|7839|05-01-1981|2850|    30| 2022-01-04|\n",
      "| 7782|  RAVI|  MANAGER|7839|06-09-1981|2450|    10| 2022-01-02|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-04-1987|3000|    20| 2022-01-02|\n",
      "| 7839|  KING|PRESIDENT|null|01-11-1981|5000|    10| 2022-01-02|\n",
      "| 7844|TURNER| SALESMAN|7698|09-08-1981|1500|    30| 2022-01-02|\n",
      "| 7876| ADAMS|    CLERK|7788|23-05-1987|1100|    20| 2022-01-03|\n",
      "| 7900| JAMES|    CLERK|7698|12-03-1981| 950|    30| 2022-01-03|\n",
      "| 7902|  FORD|  ANALYST|7566|12-03-1981|3000|    20| 2022-01-03|\n",
      "| 7934|MILLER|    CLERK|7782|01-03-1982|1300|    10| 2022-01-03|\n",
      "| 1234|SEKHAR|   doctor|7777|      null| 667|    80| 2022-01-03|\n",
      "| 7369| SMITH|    CLERK|7902|17-12-1980| 800|    20| 2022-01-04|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-02-1981|1600|    30| 2022-01-04|\n",
      "| 7521|  WARD| SALESMAN|7698|22-02-1981|1250|    30| 2022-01-04|\n",
      "| 7566| JONES|  MANAGER|7839|04-02-1981|2975|    20| 2022-01-04|\n",
      "| 7654|MARTIN| SALESMAN|7698|21-09-1981|1250|    30| 2022-01-05|\n",
      "+-----+------+---------+----+----------+----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.drop(df_emp.comm).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4e5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+----+----------+----+------+\n",
      "|empno|ename|    job| mgr|  hiredate| sal|deptno|\n",
      "+-----+-----+-------+----+----------+----+------+\n",
      "| 7369|SMITH|  CLERK|7902|17-12-1980| 800|    20|\n",
      "| 7566|JONES|MANAGER|7839|04-02-1981|2975|    20|\n",
      "| 7788|SCOTT|ANALYST|7566|19-04-1987|3000|    20|\n",
      "| 7876|ADAMS|  CLERK|7788|23-05-1987|1100|    20|\n",
      "| 7902| FORD|ANALYST|7566|12-03-1981|3000|    20|\n",
      "| 7369|SMITH|  CLERK|7902|17-12-1980| 800|    20|\n",
      "| 7566|JONES|MANAGER|7839|04-02-1981|2975|    20|\n",
      "| 7788|SCOTT|ANALYST|7566|19-04-1987|3000|    20|\n",
      "| 7876|ADAMS|  CLERK|7788|23-05-1987|1100|    20|\n",
      "| 7902| FORD|ANALYST|7566|12-03-1981|3000|    20|\n",
      "+-----+-----+-------+----+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.drop(*[\"update_date\",\"mdg\",\"comm\"]).filter(df_emp.deptno=='20').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6764fc",
   "metadata": {},
   "source": [
    "# Renaming Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b9e3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------+----+----------+----+------+\n",
      "|empno|Employee_name|      job| mgr|  hiredate| sal|deptno|\n",
      "+-----+-------------+---------+----+----------+----+------+\n",
      "| 7499|        ALLEN| SALESMAN|7698|20-02-1981|1600|    30|\n",
      "| 7566|        JONES|  MANAGER|7839|04-02-1981|2975|    20|\n",
      "| 7698|          SGR|  MANAGER|7839|05-01-1981|2850|    30|\n",
      "| 7782|         RAVI|  MANAGER|7839|06-09-1981|2450|    10|\n",
      "| 7788|        SCOTT|  ANALYST|7566|19-04-1987|3000|    20|\n",
      "| 7839|         KING|PRESIDENT|null|01-11-1981|5000|    10|\n",
      "| 7902|         FORD|  ANALYST|7566|12-03-1981|3000|    20|\n",
      "| 7499|        ALLEN| SALESMAN|7698|20-02-1981|1600|    30|\n",
      "| 7566|        JONES|  MANAGER|7839|04-02-1981|2975|    20|\n",
      "| 7698|          SGR|  MANAGER|7839|05-01-1981|2850|    30|\n",
      "| 7782|         RAVI|  MANAGER|7839|06-09-1981|2450|    10|\n",
      "| 7788|        SCOTT|  ANALYST|7566|19-04-1987|3000|    20|\n",
      "| 7839|         KING|PRESIDENT|null|01-11-1981|5000|    10|\n",
      "| 7902|         FORD|  ANALYST|7566|12-03-1981|3000|    20|\n",
      "+-----+-------------+---------+----+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.withColumnRenamed(\"ename\",\"Employee_name\").drop(*[\"update_date\",\"mdg\",\"comm\"]).filter(df_emp.sal > 1500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e41de",
   "metadata": {},
   "source": [
    "# Advanced Manipulations\n",
    "\n",
    "In order to respond to most situations, PySpark already has a number of pre-implemented functions present in the pyspark.sql.functions module\n",
    "The functions are numerous and of all types, Do not hesitate to refer to it.\n",
    "Mathematical functions(cos,sin,tan,abs,exp,log ...)\n",
    "Dataframes manipulation(Concatenate, repartitioning,Structure manipulation....)\n",
    "Statistics (avarage, variance, correlation, covariance ..)\n",
    "Date manipulation.\n",
    "Operators of relational algebra\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73059373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "| x1| x2|            cos_num|\n",
      "+---+---+-------------------+\n",
      "|  1|  2|-0.9899924966004454|\n",
      "|  2|  3|0.28366218546322625|\n",
      "|  3|  4| 0.7539022543433046|\n",
      "|  4|  5|-0.9111302618846769|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cos\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "[[1,2],[2,3],[3,4],[4,5]],\n",
    "schema=[\"x1\",\"x2\"])\n",
    "\n",
    "df=df.withColumn(\"cos_num\",cos(df.x1 + df.x2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f353c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|sum(sal)|\n",
      "+------+--------+\n",
      "|10    |17500   |\n",
      "|20    |21750   |\n",
      "|30    |18800   |\n",
      "|80    |1161    |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df=df_emp.withColumn(\"sal\",df_emp.sal.cast('int')).groupBy(\"deptno\").sum(\"sal\").orderBy(\"deptno\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cdc5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|deptno|dept count|\n",
      "+------+----------+\n",
      "|    10|         6|\n",
      "|    20|        10|\n",
      "|    30|        12|\n",
      "|    80|         2|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.groupBy(\"deptno\").count().withColumnRenamed(\"count\",\"dept count\").select(\"deptno\",\"dept count\").orderBy(\"deptno\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebc1a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|deptno|\n",
      "+------+\n",
      "|    30|\n",
      "|    20|\n",
      "|    10|\n",
      "|    80|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.groupBy(\"deptno\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee1c79",
   "metadata": {},
   "source": [
    "# UDF in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1192c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(\"float\")  # Decorator to define an UDF to return  an float\n",
    "\n",
    "def add_3(x):\n",
    "    return x*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eb02df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Price_hike|\n",
      "+----------+\n",
      "|      57.0|\n",
      "|      36.0|\n",
      "|      24.0|\n",
      "|      45.0|\n",
      "|      60.0|\n",
      "|      30.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(add_3(df.price).alias('Price_hike')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098ae93",
   "metadata": {},
   "source": [
    "# Pandas to pySpark and vice Versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78e51842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0),\n",
       " Row(city='Paris', type='Food', price=8.0),\n",
       " Row(city='Paris', type='Cloting', price=15.0),\n",
       " Row(city='Marseille', type='Food', price=20.0),\n",
       " Row(city='Lyon', type='Book', price=10.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header=[\"city\",\"type\",\"price\"]\n",
    "data=map(lambda r:(r[0], r[1], float(r[2])),\n",
    "         map(lambda x:x.split(\",\"),\n",
    "            [\"Paris,Food,19.0\",\"Marseille,Clothing,12.00\",\n",
    "             \"Paris,Food,8.00\",\"Paris,Cloting,15.00\",\n",
    "             \"Marseille,Food,20.00\",\"Lyon,Book,10.00\"\n",
    "            ]))\n",
    "df=spark.createDataFrame(data, header)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1c97075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris| Cloting| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cefa731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>type</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paris</td>\n",
       "      <td>Food</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marseille</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paris</td>\n",
       "      <td>Food</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paris</td>\n",
       "      <td>Cloting</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marseille</td>\n",
       "      <td>Food</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lyon</td>\n",
       "      <td>Book</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city      type  price\n",
       "0      Paris      Food   19.0\n",
       "1  Marseille  Clothing   12.0\n",
       "2      Paris      Food    8.0\n",
       "3      Paris   Cloting   15.0\n",
       "4  Marseille      Food   20.0\n",
       "5       Lyon      Book   10.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pd=df.toPandas()\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4cafa",
   "metadata": {},
   "source": [
    "# Pandas to Pyspark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f965b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris| Cloting| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"Pyspark-Skyhigh\").getOrCreate()\n",
    "sqlctx=SQLContext(spark)\n",
    "\n",
    "sqlctx.createDataFrame(df_pd).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e190dd8",
   "metadata": {},
   "source": [
    "# Removing null Rows (udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bf23f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     Loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "|    50| MARKETING|        |\n",
      "|    20|   FINANCE|        |\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlctx=SQLContext(spark)\n",
    "\n",
    "df=sqlctx.createDataFrame([\n",
    "    [10,'ACCOUNTING','NEW YORK'],\n",
    "    [20,'RESEARCH','DALLAS'],\n",
    "    [30,'SALES','CHICAGO'],\n",
    "    [40,'OPERATIONS','BOSTON'],\n",
    "    [50,'MARKETING',''],\n",
    "    [20,'FINANCE','']\n",
    "    ],\n",
    "    ['deptno','dname','Loc']\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e670e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+--------+\n",
      "|deptno|     dname|     Loc|category|\n",
      "+------+----------+--------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|       0|\n",
      "|    20|  RESEARCH|  DALLAS|       0|\n",
      "|    30|     SALES| CHICAGO|       0|\n",
      "|    40|OPERATIONS|  BOSTON|       0|\n",
      "|    50| MARKETING|        |       1|\n",
      "|    20|   FINANCE|        |       1|\n",
      "+------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def remove_null(x):\n",
    "    if x=='':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# Note\n",
    "udfValueCategory=udf(remove_null,IntegerType())\n",
    "df1=df.withColumn(\"category\",udfValueCategory(\"loc\").cast(IntegerType())).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1e5be7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#df1.filter(df1.category==0).show()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df1\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "#df1.filter(df1.category==0).show()\n",
    "df1.select(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98d9ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| x1| x2|   x3|\n",
      "+---+---+-----+\n",
      "|  1|  a| 23.0|\n",
      "|  3|  B|-23.0|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"Testing-SkyHigh\").getOrCreate()\n",
    "sqlctx=SQLContext(spark)\n",
    "\n",
    "df=sqlctx.createDataFrame(\n",
    "[\n",
    "    (1,\"a\",23.0)\n",
    "    ,(3,\"B\",-23.0)\n",
    "],(\"x1\",\"x2\",\"x3\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb0b622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+--------+\n",
      "| x1| x2|   x3|Category|\n",
      "+---+---+-----+--------+\n",
      "|  1|  a| 23.0|    cat1|\n",
      "|  3|  B|-23.0|     n/a|\n",
      "+---+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def valuetoCategory(value):\n",
    "    if value==1: return 'cat1'\n",
    "    elif value==2: return 'cat2'\n",
    "    else: return 'n/a'\n",
    "\n",
    "#Note: if seems that calls to udf() must be after SparkContext() is called\n",
    "\n",
    "udfcat=udf(valuetoCategory,StringType())\n",
    "df_with_cat=df.withColumn(\"Category\",udfcat(\"x1\"))\n",
    "df_with_cat.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
